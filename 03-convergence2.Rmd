---
title: "sim ranges"
author: "Tommy Jones"
date: "7/21/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = TRUE)
```

```{r libraries}
library(tidyverse)
library(tmsamples)
library(tidytext)
library(tidylda)
library(magick)

# create a common date/time for saving any elements to disk
clockmark <-
  Sys.time() %>%
  str_replace_all("[^a-zA-Z0-9]+", "-")

```


```{r gen-pars, results='hide'}
set.seed(8675309)

Nv = 5000

Nd = 10000

Nk = 25

gen_pars <- function(Nd, Nv, Nk, beta_sum, alpha_sum, alpha_shape = "flat") {
  
  z <- generate_zipf(vocab_size = Nv, magnitude = beta_sum)
  
  if (alpha_shape == "flat") {
    a <- rep(alpha_sum / Nk, Nk)
  } else {
    a <- rgamma(Nk, 1)
    
    a <- a / sum(a) * alpha_sum
  }
  
  par <- sample_parameters(alpha = a, beta = z, num_documents = Nd)
  
  par <- list(
    par = par,
    beta_sum = beta_sum,
    alpha_sum = alpha_sum,
    alpha_shape = alpha_shape
  )
}



beta_sums <- c(50, 250, 500, 1000)

alpha_sums <- c(0.5, 1, 3, 5)

alpha_shapes <- c("flat", "flat") # c("flat", "asymmetric")

doc_lengths <- c(50, 100, 200, 400)

pars <- cross3(
  alpha_sums, 
  beta_sums, 
  alpha_shapes
) %>%
  map(function(x) {
    names(x) <- c("alpha_sum", "beta_sum", "alpha_shape")
    x
  }) %>%
  map(function(x){
    gen_pars(
      Nd = Nd,
      Nv = Nv,
      Nk = Nk,
      beta_sum = x$beta_sum,
      alpha_sum = x$alpha_sum,
      alpha_shape = x$alpha_shape
    )
  }) %>%
  cross2(doc_lengths) %>%
  map(function(x){
    names(x) <- c("par", "doc_length")
    
    x
  })

# save pars to track results later
new_dir <- 
  paste0("data-derived/", clockmark, "/")

if (! dir.exists(new_dir))
  dir.create(new_dir)

save(
  pars,
  file = paste0(new_dir, "pop-pars.rds")
)

```

```{r end-to-end}
# add model number to track results later
for (j in seq_along(pars)) 
  pars[[j]]$model_num <- j


hdist <- pars %>%
  parallel::mclapply(
    function(p) {
      # pull out relevant objects
      pop_pars <- p$par$par
      
      model_name <- if (nchar(p$model_num) == 1) {
        paste0("00", p$model_num)
      } else if (nchar(p$model_num) == 2) {
        paste0("0", p$model_num)
      } else {
        as.character(p$model_num)
      }
      
      # generate documents
      doc_lengths <- rpois(
        n = nrow(pop_pars$theta),
        lambda = p$doc_length
      )
      
      docs <- sample_documents(
        theta = pop_pars$theta,
        phi = pop_pars$phi,
        doc_lengths = doc_lengths,
        threads = 1
      )
      
      # divide docs into batches
      batch_size <- 100
      
      batches <- seq(1, nrow(docs), by = batch_size)
      
      batches <- map(batches, function(x) x:min(x + batch_size - 1, nrow(docs)))
      
      # model each batch successively
      models <- vector(mode = "list", length = length(batches))
      
      z <- generate_zipf(vocab_size = ncol(docs), magnitude = p$par$beta_sum)
      
      d <- docs[batches[[1]], ]
      
      tf <- textmineR::TermDocFreq(d)
      
      vocab <- tf %>%
        filter(doc_freq > 0 & doc_freq < nrow(d) - 5) %>%
        select(term) %>%
        .[[1]]
      
      k <- nrow(pop_pars$phi) + 5 # more estimated topics than actual
      
      alpha <- 0.05 # symmetric and small alpha no matter what
      
      eta <- 0.01 # symmetric and small eta no matter what
      
      # if (p$par$alpha_shape == "flat") {
      #   alpha <- rep(p$par$alpha_sum / k, k)
      # } else {
      #   alpha <- rgamma(Nk, 1)
      #   
      #   alpha <- alpha / sum(alpha) * p$par$alpha_sum
      # }
      # 
      # eta <- z[vocab]
      
      models[[1]] <- try({
        tidylda(
          data = d[, vocab],
          k = k,
          iterations = 100,
          burnin = 75,
          alpha = alpha,
          eta = eta,
          calc_likelihood = TRUE,
          calc_r2 = TRUE,
          verbose = TRUE
        )
      })
      
      for (j in 2:length(models)) {
        
        d <- docs[batches[[j]], ]
        
        tf <- textmineR::TermDocFreq(d)
        
        vocab <- tf %>%
          filter(doc_freq > 0 & doc_freq < nrow(d) - 5) %>%
          select(term) %>%
          .[[1]]
        
        models[[j]] <- try({
          refit(
            object = models[[j - 1]],
            new_data = d[, vocab],
            iterations = 200,
            burnin = 150,
            calc_likelihood = TRUE
          )
        })
        
      }
      
      # write models to file
      save(
        models,
        file = paste0(new_dir, "m-", model_name, ".rds")
      )
      
      # tibble of relevant pars/perf metrics
      
      metrics <- models %>%
        map(function(m) {
          tibble(
            coherence = try(mean(m$summary$coherence)),
            likelihood = try(mean(
              m$log_likelihood$log_likelihood[m$log_likelihood$iteration >= 150]
            )),
            prevalence = try(mean(m$summary$prevalence))
          )
        }) %>%
        bind_rows() %>%
        mutate(
          beta_sum = p$par$beta_sum,
          alpha_sum = p$par$alpha_sum,
          alpha_shape = p$par$alpha_shape,
          avg_doc_length = p$doc_length
        )
      
      
      # hdist between model and pars
      hdist <- models %>%
        map(function(m){
          try({
            vocab <- intersect(colnames(m$beta), colnames(pop_pars$phi))
            
            h <- apply(m$beta[, vocab], 1, function(p){
              apply(pop_pars$phi[, vocab], 1, function(q){
                textmineR::CalcHellingerDist(p, q)
              })
            })
            h
          })
        })
      
      # list of hdist and tibble out
      list(
        metrics = metrics, 
        hdist = hdist
      )
      
      
    },
    mc.cores = parallel::detectCores() - 1
  )



```



```{r save}

# add info to metrics tables
for (j in seq_along(hdist)) {
  hdist[[j]]$metrics$model <- j
  
  hdist[[j]]$metrics$iteration <- 1:nrow(hdist[[j]]$metrics)
}

save(
  hdist,
  file = paste0(new_dir, "hdist.rds")
)

```

```{r visibility-metric}
# extract viz metrics
vis_metric <- 
  hdist %>% 
  map(function(x){
    apply(x$hdist[[10]], 1, function(y) median(y) / min(y))
  }) %>%
  bind_cols()

names(vis_metric) <- as.character(1:ncol(vis_metric))

# merge with parameters for generation
metrics <- 
  hdist %>%
  map(function(x) x$metrics) %>%
  bind_rows()

vis_metric <-
  vis_metric %>%
  mutate(topic = 1:n()) %>%
  pivot_longer(
    cols = matches("\\d+"),
    names_to = "model",
    values_to = "score"
  ) %>%
  mutate(model = as.numeric(model)) %>%
  left_join(
    metrics %>% filter(iteration == 10),
    by = c("model" = "model")
  )

```

```{r vis-hdist}
vis_hdist <- hdist %>% map(function(x){
  
  result <- x$hdist %>%
    map(function(y){
      
      min_hdist <- apply(y, 2, min)
      med_hdist <- apply(y, 2, median)
      score <- med_hdist / min_hdist
      
      
      out <- tibble(
        topic = seq_along(score), 
        min_hdist = min_hdist,
        med_hdist = med_hdist,
        score = score
      )
    })
  
  for(j in seq_along(result)) 
    result[[j]]$period <- j
  
  result %>% bind_rows()
})
for (j in seq_along(vis_hdist)) 
  vis_hdist[[j]]$model <- j

vis_hdist <- vis_hdist %>% bind_rows()



```

### Plot minimum hdist
```{r plot-mins}
for(j in unique(vis_hdist$model)) {
  plot(
    vis_hdist %>% 
      filter(model == j) %>% 
      ggplot() + 
      geom_line(aes(x = period, y = min_hdist, colour = factor(topic))) + 
      theme(legend.position="none") +
      ggtitle(paste("Model:", j))
  )
}
```


### Plot score

```{r plot-score}
for(j in unique(vis_hdist$model)) {
  plot(
    vis_hdist %>% 
      filter(model == j) %>% 
      ggplot() + 
      geom_line(aes(x = period, y = score, colour = factor(topic))) + 
      theme(legend.position="none") +
      ggtitle(paste("Model:", j))
  )
}
```



```{r heatmap-gifs}

if (! dir.exists("output/converge-heatmap")) {
  dir.create("output/converge-heatmap")
}

new_dir <- 
  paste0("output/converge-heatmap/",
         clockmark,
         "/"
  )

dir.create(new_dir) 

for (m in seq_along(hdist)) {
  
  dir.create(paste0(new_dir, m))
  
  # plot those suckers in a heatmap to make a gif
  for (j in seq_along(hdist[[m]]$hdist)) {
    # label with leading zeros for files in order
    lab <- if(nchar(j) == 1){
      paste0("00", j)
    } else if (nchar(j) == 2) {
      paste0("0", j)
    } else {
      as.character(j)
    }
    
    png(paste0(new_dir, m, "/", lab, ".png"))
    heatmap(
      hdist[[m]]$hdist[[j]],
      Rowv = NA,
      Colv = NA,
      main = paste0("Model: ", m, "; t = ", j)
    )
    dev.off()
    
  }
  
  ### compile images into animated gif
  # ## list file names and read in
  # imgs <- list.files(paste0(new_dir, m), full.names = TRUE)
  # 
  # img_list <- lapply(imgs, image_read)
  # 
  # ## join the images together
  # img_joined <- image_join(img_list)
  # 
  # ## animate at 2 frames per second
  # img_animated <- image_animate(img_joined, fps = 10)
  # 
  # ## view animated image
  # # img_animated
  # 
  # ## save to disk
  # lab2 <- if(nchar(m) == 1){
  #   paste0("00", m)
  # } else if (nchar(m) == 2) {
  #   paste0("0", m)
  # } else {
  #   as.character(m)
  # }
  # 
  # image_write(image = img_animated,
  #             path = paste0(new_dir, lab2, ".gif"))
  
}



```

