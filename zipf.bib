@article{undefined, 
title = {{Assessing Language Models with Scaling Properties}}, 
author = {Takahashi, Shuntaro and Tanaka-Ishii, Kumiko}, 
journal = {arXiv}, 
eprint = {1804.08881}, 
abstract = {{Language models have primarily been evaluated with perplexity. While perplexity quantifies the most comprehensible prediction performance, it does not provide qualitative information on the success or failure of models. Another approach for evaluating language models is thus proposed, using the scaling properties of natural language. Five such tests are considered, with the first two accounting for the vocabulary population and the other three for the long memory of natural language. The following models were evaluated with these tests: n-grams, probabilistic context-free grammar (PCFG), Simon and Pitman-Yor (PY) processes, hierarchical PY, and neural language models. Only the neural language models exhibit the long memory properties of natural language, but to a limited degree. The effectiveness of every test of these models is also discussed.}}, 
local-url = {file://localhost/Users/tommy/Google%20Drive/gmu/_dissertation/references/2020-11-15/1804.08881.pdf}, 
year = {2018}
}
@book{boydgraber2017applications, 
title = {{Applications of Topic Models}}, 
author = {Boyd-Graber, Jordan and Hu, Yuening and Mimno, David}, 
volume = {11}, 
publisher = {now Publishers Incorporated}, 
local-url = {file://localhost/Users/tommy/Google%20Drive/gmu/_dissertation/references/2017_fntir_tm_applications.pdf}, 
year = {2017}
}
@article{altmann2015laws, 
title = {{Statistical laws in linguistics}}, 
author = {Altmann, Eduardo G and Gerlach, Martin}, 
journal = {arXiv}, 
doi = {10.1007/978-3-319-24403-7\_2}, 
eprint = {1502.03296}, 
abstract = {{Zipf's law is just one out of many universal laws proposed to describe statistical regularities in language. Here we review and critically discuss how these laws can be statistically interpreted, fitted, and tested (falsified). The modern availability of large databases of written text allows for tests with an unprecedent statistical accuracy and also a characterization of the fluctuations around the typical behavior. We find that fluctuations are usually much larger than expected based on simplifying statistical assumptions (e.g., independence and lack of correlations between observations).These simplifications appear also in usual statistical tests so that the large fluctuations can be erroneously interpreted as a falsification of the law. Instead, here we argue that linguistic laws are only meaningful (falsifiable) if accompanied by a model for which the fluctuations can be computed (e.g., a generative model of the text). The large fluctuations we report show that the constraints imposed by linguistic laws on the creativity process of text generation are not as tight as one could expect.}}, 
local-url = {file://localhost/Users/tommy/Google%20Drive/gmu/_dissertation/references/download%202020-08-09/1502.03296.pdf}, 
year = {2015}
}
@article{gillespie2015, 
title = {{Fitting Heavy Tailed Distributions: The \{poweRlaw\} Package}}, 
author = {Gillespie, Colin S.}, 
journal = {Journal of Statistical Software}, 
url = {http://www.jstatsoft.org/v64/i02/}, 
pages = {1---16}, 
number = {2}, 
volume = {64}, 
local-url = {file://localhost/Users/tommy/Google%20Drive/gmu/_dissertation/references/2020-12-31/v64i02.pdf}, 
year = {2015}
}
@article{gerlach2014, 
title = {{Scaling laws and fluctuations in the statistics of word frequencies}}, 
author = {Gerlach, Martin and Altmann, Eduardo G}, 
journal = {New Journal of Physics}, 
issn = {1367-2630}, 
doi = {10.1088/1367-2630/16/11/113010}, 
eprint = {1406.4441}, 
abstract = {{In this paper, we combine statistical analysis of written texts and simple stochastic models to explain the appearance of scaling laws in the statistics of word frequencies. The average vocabulary of an ensemble of fixed-length texts is known to scale sublinearly with the total number of words (Heaps' law). Analyzing the fluctuations around this average in three large databases (Google-ngram, English Wikipedia, and a collection of scientific articles), we find that the standard deviation scales linearly with the average (Taylorʼs law), in contrast to the prediction of decaying fluctuations obtained using simple sampling arguments. We explain both scaling laws (Heaps' and Taylor) by modeling the usage of words using a Poisson process with a fat-tailed distribution of word frequencies (Zipfʼs law) and topic-dependent frequencies of individual words (as in topic models). Considering topical variations lead to quenched averages, turn the vocabulary size a non-self-averaging quantity, and explain the empirical observations. For the numerous practical applications relying on estimations of vocabulary size, our results show that uncertainties remain large even for long texts. We show how to account for these uncertainties in measurements of lexical richness of texts with different lengths.}}, 
pages = {113010}, 
number = {11}, 
volume = {16}, 
local-url = {file://localhost/Users/tommy/Google%20Drive/gmu/_dissertation/references/2020-11-15/Gerlach_2014_New_J._Phys._16_113010.pdf}, 
year = {2014}
}
@article{goldwater2011zipf, 
title = {{Producing Power-Law Distributions and Damping Word Frequencies with Two-Stage Language Models}}, 
author = {Goldwater, Sharon and Griffiths, Thomas L. and Johnson, Mark}, 
journal = {Journal of Machine Learning Research}, 
journaltitle = {Journal of Machine Learning Research}, 
volume = {12}, 
year = {2011}
}
@article{10.1371/journal.pone.0014139, 
title = {{Zipf's Law Leads to Heaps' Law: Analyzing Their Relation in Finite-Size Systems}}, 
author = {Lü, Linyuan and Zhang, Zi-Ke and Zhou, Tao}, 
journal = {PLoS ONE}, 
doi = {10.1371/journal.pone.0014139}, 
pmid = {21152034}, 
pmcid = {PMC2996287}, 
eprint = {1002.3861}, 
abstract = {{Zipf's law and Heaps' law are observed in disparate complex systems. Of particular interests, these two laws often appear together. Many theoretical models and analyses are performed to understand their co-occurrence in real systems, but it still lacks a clear picture about their relation. We show that the Heaps' law can be considered as a derivative phenomenon if the system obeys the Zipf's law. Furthermore, we refine the known approximate solution of the Heaps' exponent provided the Zipf's exponent. We show that the approximate solution is indeed an asymptotic solution for infinite systems, while in the finite-size system the Heaps' exponent is sensitive to the system size. Extensive empirical analysis on tens of disparate systems demonstrates that our refined results can better capture the relation between the Zipf's and Heaps' exponents. The present analysis provides a clear picture about the relation between the Zipf's law and Heaps' law without the help of any specific stochastic model, namely the Heaps' law is indeed a derivative phenomenon from the Zipf's law. The presented numerical method gives considerably better estimation of the Heaps' exponent given the Zipf's exponent and the system size. Our analysis provides some insights and implications of real complex systems. For example, one can naturally obtained a better explanation of the accelerated growth of scale-free networks.}}, 
pages = {e14139}, 
number = {12}, 
volume = {5}, 
local-url = {file://localhost/Users/tommy/Google%20Drive/gmu/_dissertation/references/2020-11-09/journal.pone.0014139.PDF}, 
year = {2010}
}
@article{10.1080/09296170902850358, 
title = {{Mandelbrot's Model for Zipf's Law: Can Mandelbrot's Model Explain Zipf's Law for Language?}}, 
author = {Manin, D. Yu.}, 
journal = {Journal of Quantitative Linguistics}, 
issn = {0929-6174}, 
doi = {10.1080/09296170902850358}, 
abstract = {{Zipf's law states that if words of a language are sorted in the order of decreasing frequency of usage, a word's frequency is inversely proportional to its rank, or sequence number in the list. The Zipf-Mandelbrot law is a more general formula that provides a better fit in the low-rank region. Among several models aimed at explaining this effect, Mandelbrot's model is one of the best known. It derives Zipf's law as a result of the optimization of information/cost ratio, but leads to an unrealistic view of texts as random character sequences. In this article, a new modification of the model is proposed that is free from this drawback and allows the optimal information/cost ratio to be achieved via language evolution. It is demonstrated that the Zipf-Mandelbrot formula follows from this model, but its two parameters are not independent. As a result, the formula cannot convincingly be fitted to the actual word frequency distributions.}}, 
pages = {274--285}, 
number = {3}, 
volume = {16}, 
local-url = {file://localhost/Users/tommy/Google%20Drive/gmu/_dissertation/references/2020-11-09/Mandelbrot%20s%20Model%20for%20Zipf%20s%20Law%20Can%20Mandelbrot%20s%20Model%20Explain%20Zipf%20s%20Law%20for%20Language.pdf}, 
year = {2009}
}
@article{cancho2003, 
title = {{Least effort and the origins of scaling in human language}}, 
author = {Cancho, Ramon Ferrer i and Sole, Ricard V.}, 
journal = {Proceedings of the National Academy of Sciences}, 
pages = {788---791}, 
number = {3}, 
volume = {100}, 
local-url = {file://localhost/Users/tommy/Google%20Drive/gmu/_dissertation/references/2020-12-31/788.full.pdf}, 
year = {2003}
}
@inproceedings{ha2002zipf, 
author = {Ha, Le Quan and Sicilia-Garcia, E. I. and Ming, Ji and Smith, F. J.}, 
title = {{Extension of Zipf's law to words and phrases}}, 
booktitle = {COLING 2002: The 19th International Conference on Computational Linguistics}, 
doi = {10.3115/1072228.1072345}, 
abstract = {{Zipf's law states that the frequency of word tokens in a large corpus of natural language is inversely proportional to the rank. The law is investigated for two languages English and Mandarin and for n-gram word phrases as well as for single words. The law for single words is shown to be valid only for high frequency words. However, when single word and n-gram phrases are combined together in one list and put in order of frequency the combined list follows Zipf's law accurately for all words and phrases, down to the lowest frequencies in both languages. The Zipf curves for the two languages are then almost identical.}}, 
local-url = {file://localhost/Users/tommy/Documents/Papers%20Library/1072228.1072345.pdf}, 
year = {2002}
}
@book{zipf1949, 
title = {{Human behavior and the principle of least effort.}}, 
author = {Zipf, George Kingsley}, 
abstract = {{Subtitled "An introduction to human ecology," this work attempts systematically to treat "least effort" (and its derivatives) as the principle underlying a multiplicity of individual and collective behaviors, variously but regularly distributed. The general orientation is quantitative, and the principle is widely interpreted and applied. After a brief elaboration of principles and a brief summary of pertinent studies (mostly in psychology), Part One (Language and the structure of the personality) develops 8 chapters on its theme, ranging from regularities within language per se to material on individual psychology. Part Two (Human relations: a case of intraspecies balance) contains chapters on "The economy of geography," "Intranational and international cooperation and conflict," "The distribution of economic power and social status," and "Prestige values and cultural vogues"—all developed in terms of the central theme. 20 pages of references with some annotation, keyed to the index. (PsycINFO Database Record (c) 2016 APA, all rights reserved)}}, 
series = {Human behavior and the principle of least effort.}, 
publisher = {Addison-Wesley Press}, 
address = {Oxford,  England}, 
note = {Subtitled "An introduction to human ecology," this work attempts systematically to treat "least effort" (and its derivatives) as the principle underlying a multiplicity of individual and collective behaviors, variously but regularly distributed. The general orientation is quantitative, and the principle is widely interpreted and applied. After a brief elaboration of principles and a brief summary of pertinent studies (mostly in psychology), Part One (Language and the structure of the personality) develops 8 chapters on its theme, ranging from regularities within language per se to material on individual psychology. Part Two (Human relations: a case of intraspecies balance) contains chapters on "The economy of geography," "Intranational and international cooperation and conflict," "The distribution of economic power and social status," and "Prestige values and cultural vogues"—all developed in terms of the central theme. 20 pages of references with some annotation, keyed to the index. (PsycINFO Database Record (c) 2016 APA, all rights reserved)}, 
year = {1949}
}