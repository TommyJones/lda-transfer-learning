@article{dieng2020tem, 
title = {{Topic Modeling in Embedding Spaces}}, 
author = {Dieng, Adji B. and Ruiz, Francisco J. R. and Blei, David M.}, 
journal = {Transactions of the Association for Computational Linguistics}, 
doi = {10.1162/tacl\_a\_00325}, 
abstract = {{Topic modeling analyzes documents to learn meaningful patterns of words. However, existing topic models fail to learn interpretable topics when working with large and heavy-tailed vocabularies. To this end, we develop the embedded topic model (etm), a generative model of documents that marries traditional topic models with word embeddings. More specifically, the etm models each word with a categorical distribution whose natural parameter is the inner product between the word’s embedding and an embedding of its assigned topic. To fit the etm, we develop an efficient amortized variational inference algorithm. The etm discovers interpretable topics even with large vocabularies that include rare words and stop words. It outperforms existing document models, such as latent Dirichlet allocation, in terms of both topic quality and predictive performance.}}, 
pages = {439--453}, 
volume = {8}, 
local-url = {file://localhost/Users/tommy/Google%20Drive/gmu/_dissertation/references/2020-12-25/DiengRuizBlei2020a.pdf}, 
year = {2020}
}
@article{hayashasymptoticlda, 
title = {{The Exact Asymptotic Form of Bayesian Generalization Error in Latent Dirichlet Allocation}}, 
author = {Hayashi, Naoki}, 
journal = {arXiv}, 
eprint = {2008.01304}, 
abstract = {{Latent Dirichlet allocation (LDA) obtains essential information from data by using Bayesian inference. It is applied to knowledge discovery via dimension reducing and clustering in many fields. However, its generalization error had not been yet clarified since it is a singular statistical model where there is no one to one map from parameters to probability distributions. In this paper, we give the exact asymptotic form of its generalization error and marginal likelihood, by theoretical analysis of its learning coefficient using algebraic geometry. The theoretical result shows that the Bayesian generalization error in LDA is expressed in terms of that in matrix factorization and a penalty from the simplex restriction of LDA's parameter region.}}, 
local-url = {file://localhost/Users/tommy/Google%20Drive/gmu/_dissertation/references/download%202020-08-07/2008.01304.pdf}, 
year = {2020}
}
@inproceedings{pietilainen2020coherence, 
author = {Pietilainen, Pirkko}, 
title = {{Properties of Semantic Coherence Measures - Case of Topic Models}}, 
booktitle = {SEMAPRO 2020 : The Fourteenth International Conference on Advances in Semantic Processing}, 
isbn = {9781612088136}, 
local-url = {file://localhost/Users/tommy/Documents/Papers%20Library/semapro_2020_2_30_30015.pdf}, 
year = {2020}
}
@inproceedings{wangneuraltopics, 
author = {Wang, Xinyi and Yang, Yi}, 
title = {{Neural Topic Model with Attention for Supervised Learning}}, 
booktitle = {Proceedings of the 23 rd International Conference on Artifi- cial Intelligence and Statistics (AISTATS) 2020}, 
local-url = {file://localhost/Users/tommy/Google%20Drive/gmu/_dissertation/references/download%202020-06-23/wang20c.pdf}, 
year = {2020}
}
@inproceedings{riegerldaprototype, 
author = {Rieger, Jonas and Rahnenführer, Jörg and Jentsch, Carsten}, 
title = {{Improving Latent Dirichlet Allocation: On Reliability of the Novel Method LDAPrototype}}, 
booktitle = {Natural Language Processing and Information Systems, 25th International Conference on Applications of Natural Language to Information Systems, NLDB 2020, Saarbrücken, Germany, June 24–26, 2020, Proceedings}, 
isbn = {9783030513092}, 
doi = {10.1007/978-3-030-51310-8\_11}, 
abstract = {{A large number of applications in text data analysis use the Latent Dirichlet Allocation (LDA) as one of the most popular methods in topic modeling. Although the instability of the LDA is mentioned sometimes, it is usually not considered systematically. Instead, an LDA is often selected from a small set of LDAs using heuristic means or human codings. Then, conclusions are often drawn based on the to some extent arbitrarily selected model. We present the novel method LDAPrototype, which takes the instability of the LDA into account, and show that by systematically selecting an LDA it improves the reliability of the conclusions drawn from the result and thus provides better reproducibility. The improvement coming from this selection criterion is unveiled by applying the proposed methods to an example corpus consisting of texts published in a German quality newspaper over one month.}}, 
pages = {118--125}, 
local-url = {file://localhost/Users/tommy/Google%20Drive/gmu/_dissertation/references/download%202020-06-23/Rieger2020_Chapter_ImprovingLatentDirichletAlloca.pdf}, 
year = {2020}
}
@article{maier2020document, 
title = {{How document sampling and vocabulary pruning affect the results of topic models}}, 
author = {Maier, Daniel and Niekler, Andreas and Wiedemann, Gregor and Stoltenberg, Daniela}, 
journal = {Computational Communication Research}, 
pages = {139---152}, 
number = {2}, 
volume = {2}, 
local-url = {file://localhost/Users/tommy/Google%20Drive/gmu/_dissertation/references/2020-12-29/CCR_R_1_maier_et_al_2019_11_19.pdf}, 
year = {2020}
}
@article{maennelexactmarginal, 
title = {{Exact marginal inference in Latent Dirichlet Allocation}}, 
author = {Maennel, Hartmut}, 
journal = {arXiv}, 
eprint = {2004.00115}, 
abstract = {{Assume we have potential "causes" \$z\textbackslashin Z\$, which produce "events" \$w\$ with known probabilities \$\textbackslashbeta(w|z)\$. We observe \$w\_1,w\_2,...,w\_n\$, what can we say about the distribution of the causes? A Bayesian estimate will assume a prior on distributions on \$Z\$ (we assume a Dirichlet prior) and calculate a posterior. An average over that posterior then gives a distribution on \$Z\$, which estimates how much each cause \$z\$ contributed to our observations. This is the setting of Latent Dirichlet Allocation, which can be applied e.g. to topics "producing" words in a document. In this setting usually the number of observed words is large, but the number of potential topics is small. We are here interested in applications with many potential "causes" (e.g. locations on the globe), but only a few observations. We show that the exact Bayesian estimate can be computed in linear time (and constant space) in \$|Z|\$ for a given upper bound on \$n\$ with a surprisingly simple formula. We generalize this algorithm to the case of sparse probabilities \$\textbackslashbeta(w|z)\$, in which we only need to assume that the tree width of an "interaction graph" on the observations is limited. On the other hand we also show that without such limitation the problem is NP-hard.}}, 
year = {2020}
}
@inproceedings{kalepallicompareldalsa, 
author = {Kalepalli, Yaswanth and Tasneem, Shaik and Teja, Pasupuleti Durga Phani and Manne, Dr. Suneetha}, 
title = {{Effective Comparison of LDA with LSA for  Topic Modelling}}, 
booktitle = {Proceedings of the International Conference on Intelligent Computing and Control Systems}, 
isbn = {978-1-7281-4876-2}, 
local-url = {file://localhost/Users/tommy/Google%20Drive/gmu/_dissertation/references/download%202020-06-23/09120888.pdf}, 
year = {2020}
}
@article{10.1016/j.infsof.2020.106411, 
title = {{A Systematic Comparison of Search-Based Approaches for LDA Hyperparameter Tuning}}, 
author = {Panichella, Annibale}, 
journal = {Information and Software Technology}, 
issn = {0950-5849}, 
doi = {10.1016/j.infsof.2020.106411}, 
abstract = {{Context:Latent Dirichlet Allocation (LDA) has been successfully used in the literature to extract topics from software documents and support developers in various software engineering tasks. While LDA has been mostly used with default settings, previous studies showed that default hyperparameter values generate sub-optimal topics from software documents. Objective: Recent studies applied meta-heuristic search (mostly evolutionary algorithms) to configure LDA in an unsupervised and automated fashion. However, previous work advocated for different meta-heuristics and surrogate metrics to optimize. The objective of this paper is to shed light on the influence of these two factors when tuning LDA for SE tasks. Method:We empirically evaluated and compared seven state-of-the-art meta-heuristics and three alternative surrogate metrics (i.e., fitness functions) to solve the problem of identifying duplicate bug reports with LDA. The benchmark consists of ten real-world and open-source projects from the Bench4BL dataset. Results:Our results indicate that (1) meta-heuristics are mostly comparable to one another (except for random search and CMA-ES), and (2) the choice of the surrogate metric impacts the quality of the generated topics and the tuning overhead. Furthermore, calibrating LDA helps identify twice as many duplicates than untuned LDA when inspecting the top five past similar reports. Conclusion:No meta-heuristic and/or fitness function outperforms all the others, as advocated in prior studies. However, we can make recommendations for some combinations of meta-heuristics and fitness functions over others for practical use. Future work should focus on improving the surrogate metrics used to calibrate/tune LDA in an unsupervised fashion.}}, 
pages = {106411}, 
local-url = {file://localhost/Users/tommy/Google%20Drive/gmu/_dissertation/references/2020-09-14/1-s2.0-S0950584920300069-main.pdf}, 
year = {2020}
}
@inproceedings{panigrahi2019word2sense, 
author = {Panigrahi, Abhishek and Simhadri, Harsha Vardhan and Bhattacharyya, Chiranjib}, 
title = {{Word2Sense : Sparse Interpretable Word Embeddings}}, 
booktitle = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics}, 
pages = {5692---5705}, 
local-url = {file://localhost/Users/tommy/Google%20Drive/gmu/_dissertation/references/2020-12-29/word2sense.pdf}, 
year = {2019}
}
@article{xlm-r, 
title = {{Unsupervised Cross-lingual Representation Learning at Scale}}, 
author = {Conneau, Alexis and Khandelwal, Kartikay and Goyal, Naman and Chaudhary, Vishrav and Wenzek, Guillaume and Guzmán, Francisco and Grave, Edouard and Ott, Myle and Zettlemoyer, Luke and Stoyanov, Veselin}, 
journal = {arXiv}, 
eprint = {1911.02116}, 
abstract = {{This paper shows that pretraining multilingual language models at scale leads to significant performance gains for a wide range of cross-lingual transfer tasks. We train a Transformer-based masked language model on one hundred languages, using more than two terabytes of filtered CommonCrawl data. Our model, dubbed XLM-R, significantly outperforms multilingual BERT (mBERT) on a variety of cross-lingual benchmarks, including +14.6\% average accuracy on XNLI, +13\% average F1 score on MLQA, and +2.4\% F1 score on NER. XLM-R performs particularly well on low-resource languages, improving 15.7\% in XNLI accuracy for Swahili and 11.4\% for Urdu over previous XLM models. We also present a detailed empirical analysis of the key factors that are required to achieve these gains, including the trade-offs between (1) positive transfer and capacity dilution and (2) the performance of high and low resource languages at scale. Finally, we show, for the first time, the possibility of multilingual modeling without sacrificing per-language performance; XLM-R is very competitive with strong monolingual models on the GLUE and XNLI benchmarks. We will make our code, data and models publicly available.}}, 
local-url = {file://localhost/Users/tommy/Google%20Drive/gmu/_dissertation/references/2020-12-25/1911.02116.pdf}, 
year = {2019}
}
@article{roberts2019stm, 
title = {{stm : An R Package for Structural Topic Models}}, 
author = {Roberts, Margaret E and Stewart, Brandon M and Tingley, Dustin}, 
journal = {Journal of Statistical Software}, 
doi = {10.18637/jss.v091.i02}, 
number = {2}, 
volume = {91}, 
local-url = {file://localhost/Users/tommy/Google%20Drive/gmu/_dissertation/references/2020-12-25/stmVignette.pdf}, 
year = {2019}
}
@article{mazaruralowresource, 
title = {{Semantic representations for under-resourced languages}}, 
author = {Mazarura, Jocelyn and Waal, Alta de and Villiers, Pieter de}, 
doi = {10.1145/3351108.3351133}, 
pages = {1--10}, 
local-url = {file://localhost/Users/tommy/Documents/Papers%20Library/a24-mazarura.pdf}, 
year = {2019}
}
@unpublished{ruderneuraltransfer, 
author = {Ruder, Sebastian}, 
title = {{Neural Transfer Learning for Natural Language Processing}}, 
address = {NATIONAL UNIVERSITY OF IRELAND, GALWAY}, 
type = {PhD Thesis}, 
year = {2019}
}
@article{dosschoosek, 
title = {{Inference for the Number of Topics in the Latent Dirichlet Allocation Model via Bayesian Mixture Modeling}}, 
author = {Chen, Zhe and Doss, Hani}, 
journal = {Journal of Computational and Graphical Statistics}, 
local-url = {file://localhost/Users/tommy/Google%20Drive/gmu/_dissertation/references/download%202020-06-22/10160128.pdf}, 
year = {2019}
}
@article{takahashiscaling, 
title = {{Evaluating Computational Language Models with Scaling Properties of Natural Language}}, 
author = {Takahashi, Shuntaro and Tanaka-Ishii, Kumiko}, 
journal = {Computational Linguistics}, 
issn = {0891-2017}, 
doi = {10.1162/coli\_a\_00355}, 
abstract = {{In this article, we evaluate computational models of natural language with respect to the universal statistical behaviors of natural language. Statistical mechanical analyses have revealed that natural language text is characterized by scaling properties, which quantify the global structure in the vocabulary population and the long memory of a text. We study whether five scaling properties (given by Zipf’s law, Heaps’ law, Ebeling’s method, Taylor’s law, and long-range correlation analysis) can serve for evaluation of computational models. Specifically, we test n-gram language models, a probabilistic context-free grammar, language models based on Simon/Pitman-Yor processes, neural language models, and generative adversarial networks for text generation. Our analysis reveals that language models based on recurrent neural networks with a gating mechanism (i.e., long short-term memory; a gated recurrent unit; and quasi-recurrent neural networks) are the only computational models that can reproduce the long memory behavior of natural language. Furthermore, through comparison with recently proposed model-based evaluation methods, we find that the exponent of Taylor’s law is a good indicator of model quality.}}, 
pages = {481--513}, 
number = {3}, 
volume = {45}, 
local-url = {file://localhost/Users/tommy/Documents/Papers%20Library/Takahashi-Evaluating%20Computational%20Language%20Models%20with%20Scaling%20Properties%20of%20Natural%20Language-2019-Computational%20Linguistics.pdf}, 
year = {2019}
}
@phdthesis{shi2019thesis, 
title = {{A Principled Approach to the Evaluation of Topic Modeling Algorithms}}, 
author = {Shi, Hanyu}, 
local-url = {file://localhost/Users/tommy/Google%20Drive/gmu/_dissertation/references/2020-11-16/principled%20approach.pdf}, 
year = {2019}
}
@inproceedings{shi2019eval, 
author = {Shi, Hanyu and Gerlach, Martin and Diersen, Isabel and Downey, Doug and Amaral, Luis A N}, 
title = {{A new evaluation framework for topic modeling algorithms based on synthetic corpora}}, 
booktitle = {Proceedings of Machine Learning Research}, 
url = {http://proceedings.mlr.press/v89/shi19a.html}, 
abstract = {{Topic models are in widespread use in natural language processing and beyond. Here, we propose a new framework for the evaluation of probabilistic topic modeling algorithms based on synthetic corpora containing an unambiguously defined ground truth topic structure. The major innovation of our approach is the ability to quantify the agreement between the planted and inferred topic structures by comparing the assigned topic labels at the level of the tokens. In experiments, our approach yields novel insights about the relative strengths of topic models as corpus characteristics vary, and the first evidence of an "undetectable phase" for topic models when the planted structure is weak. We also establish the practical relevance of the insights gained for synthetic corpora by predicting the performance of topic modeling algorithms in classification tasks in real-world corpora.}}, 
pages = {816---826}, 
volume = {89}, 
publisher = {PLMR}, 
local-url = {file://localhost/Users/tommy/Google%20Drive/gmu/_dissertation/references/2020-11-16/shi19a.pdf}, 
year = {2019}
}
@article{george2018hyperparameters, 
title = {{Principled Selection of Hyperparameters in the Latent Dirichlet Allocation Model}}, 
author = {George, Clint P. and Doss, Hani}, 
journal = {Journal of Machine Learnign Research}, 
journaltitle = {Journal of Machine Learnign Research}, 
pages = {5937--5974}, 
number = {1}, 
volume = {18}, 
local-url = {file://localhost/Users/tommy/Documents/Papers%20Library/principled-hyperparameters.pdf}, 
year = {2018}
}
@inproceedings{schofield2017stopwords, 
author = {Schofield, Alexandra and Magnusson, Måns and Mimno, David}, 
title = {{Pulling Out the Stops: Rethinking Stopword Removal for Topic Models}}, 
booktitle = {Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers}, 
doi = {10.18653/v1/e17-2069}, 
url = {https://www.aclweb.org/anthology/E17-2069}, 
pages = {432--436}, 
year = {2017}
}
@article{altman2017entropy, 
title = {{Generalized entropies and the similarity of texts}}, 
author = {Altmann, Eduardo G and Dias, Laércio and Gerlach, Martin}, 
journal = {Journal of Statistical Mechanics: Theory and Experiment}, 
doi = {10.1088/1742-5468/aa53f5}, 
pages = {014002}, 
number = {1}, 
volume = {2017}, 
local-url = {file://localhost/Users/tommy/Google%20Drive/gmu/_dissertation/references/download%202020-08-07/Altmann_2017_J._Stat._Mech._2017_014002.pdf}, 
year = {2017}
}
@article{srivastava2017, 
title = {{Autoencoding variational inference for topic models}}, 
author = {Srivastava, Akash and Sutton, Charles}, 
journal = {arXiv preprint arXiv:1703.01488}, 
local-url = {file://localhost/Users/tommy/Google%20Drive/gmu/_dissertation/references/2020-12-27/1703.01488.pdf}, 
year = {2017}
}
@book{boydgraber2017applications, 
title = {{Applications of Topic Models}}, 
author = {Boyd-Graber, Jordan and Hu, Yuening and Mimno, David}, 
volume = {11}, 
publisher = {now Publishers Incorporated}, 
local-url = {file://localhost/Users/tommy/Google%20Drive/gmu/_dissertation/references/2017_fntir_tm_applications.pdf}, 
year = {2017}
}
@inproceedings{arora2016, 
author = {Arora, Sanjeev and Ge, Rong and Koehler, Frederic and Ma, Tengyu and Moitra, Ankur}, 
title = {{Provable Algorithms for Inference in Topic Models}}, 
booktitle = {Proceedings of the 33rd International Conference on Machine Learning}, 
pages = {2859–2867}, 
publisher = {PLMR}, 
local-url = {file://localhost/Users/tommy/Google%20Drive/gmu/_dissertation/references/2020-12-31/arorab16.pdf}, 
year = {2016}
}
@inproceedings{undefined, 
author = {Hsu, Wei-Shou and Poupart, Pascal}, 
title = {{Online Bayesian Moment Matching for Topic Modeling with Unknown Number of Topics}}, 
booktitle = {Advances In Neural Information Processing Systems}, 
pages = {4536---4544}, 
local-url = {file://localhost/Users/tommy/Google%20Drive/gmu/_dissertation/references/2020-12-31/NIPS-2016-online-bayesian-moment-matching-for-topic-modeling-with-unknown-number-of-topics-Paper.pdf}, 
year = {2016}
}
@article{roberts2016textmodel, 
title = {{A Model of Text for Experimentation in the Social Sciences}}, 
author = {Roberts, Margaret E and Stewart, Brandon M and Airoldi, Edoardo M}, 
journal = {Journal of the American Statistical Association}, 
issn = {0162-1459}, 
doi = {10.1080/01621459.2016.1141684}, 
abstract = {{Statistical models of text have become increasingly popular in statistics and computer science as a method of exploring large document collections. Social scientists often want to move beyond exploration, to measurement and experimentation, and make inference about social and political processes that drive discourse and content. In this article, we develop a model of text data that supports this type of substantive research. Our approach is to posit a hierarchical mixed membership model for analyzing topical content of documents, in which mixing weights are parameterized by observed covariates. In this model, topical prevalence and topical content are specified as a simple generalized linear model on an arbitrary number of document-level covariates, such as news source and time of release, enabling researchers to introduce elements of the experimental design that informed document collection into the model, within a generally applicable framework. We demonstrate the proposed methodology by analyzing a collection of news reports about China, where we allow the prevalence of topics to evolve over time and vary across newswire services. Our methods quantify the effect of news wire source on both the frequency and nature of topic coverage. Supplementary materials for this article are available online.}}, 
pages = {988--1003}, 
number = {515}, 
volume = {111}, 
local-url = {file://localhost/Users/tommy/Documents/Papers%20Library/Roberts-A%20Model%20of%20Text%20for%20Experimentation%20in%20the%20Social%20Sciences-2016-Journal%20of%20the%20American%20Statistical%20Association.pdf}, 
year = {2016}
}
@article{chen2015warplda, 
title = {{WarpLDA: a Cache Efficient O(1) Algorithm for Latent Dirichlet Allocation}}, 
author = {Chen, Jianfei and Li, Kaiwei and Zhu, Jun and Chen, Wenguang}, 
journal = {arXiv}, 
eprint = {1510.08628}, 
abstract = {{Developing efficient and scalable algorithms for Latent Dirichlet Allocation (LDA) is of wide interest for many applications. Previous work has developed an O(1) Metropolis-Hastings sampling method for each token. However, the performance is far from being optimal due to random accesses to the parameter matrices and frequent cache misses. In this paper, we first carefully analyze the memory access efficiency of existing algorithms for LDA by the scope of random access, which is the size of the memory region in which random accesses fall, within a short period of time. We then develop WarpLDA, an LDA sampler which achieves both the best O(1) time complexity per token and the best O(K) scope of random access. Our empirical results in a wide range of testing conditions demonstrate that WarpLDA is consistently 5-15x faster than the state-of-the-art Metropolis-Hastings based LightLDA, and is comparable or faster than the sparsity aware F+LDA. With WarpLDA, users can learn up to one million topics from hundreds of millions of documents in a few hours, at an unprecedentedly throughput of 11G tokens per second.}}, 
local-url = {file://localhost/Users/tommy/Google%20Drive/gmu/_dissertation/references/download%202020-06-22/1510.08628.pdf}, 
year = {2015}
}
@article{xiong2015coherence, 
title = {{Topic-Based Coherence Modeling for Statistical Machine Translation}}, 
author = {Xiong, Deyi and Zhang, Min and Wang, Xing}, 
journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing}, 
issn = {2329-9290}, 
doi = {10.1109/taslp.2015.2395254}, 
abstract = {{Coherence that ties sentences of a text into a meaningfully connected structure is of great importance to text generation and translation. In this paper, we propose topic-based coherence models to produce coherence for document translation, in terms of the continuity of sentence topics in a text. We automatically extract a coherence chain for each source text to be translated. Based on the extracted source coherence chain, we adopt a maximum entropy classifier to predict the target coherence chain that defines a linear topic structure for the target document. We build two topic-based coherence models on the predicted target coherence chain: 1) a word level coherence model that helps the decoder select coherent word translations and 2) a phrase level coherence model that guides the decoder to select coherent phrase translations. We integrate the two models into a state-of-the-art phrase-based machine translation system. Experiments on large-scale training data show that our coherence models achieve substantial improvements over both the baseline and models that are built on either document topics or sentence topics obtained under the assumption of direct topic correspondence between the source and target side. Additionally, further evaluations on translation outputs suggest that target translations generated by our coherence models are more coherent and similar to reference translations than those generated by the baseline.}}, 
pages = {483--493}, 
number = {3}, 
volume = {23}, 
local-url = {file://localhost/Users/tommy/Google%20Drive/gmu/_dissertation/references/download%202020-06-22/6187-31357-1-PB.pdf}, 
year = {2015}
}
@article{altmann2015laws, 
title = {{Statistical laws in linguistics}}, 
author = {Altmann, Eduardo G and Gerlach, Martin}, 
journal = {arXiv}, 
doi = {10.1007/978-3-319-24403-7\_2}, 
eprint = {1502.03296}, 
abstract = {{Zipf's law is just one out of many universal laws proposed to describe statistical regularities in language. Here we review and critically discuss how these laws can be statistically interpreted, fitted, and tested (falsified). The modern availability of large databases of written text allows for tests with an unprecedent statistical accuracy and also a characterization of the fluctuations around the typical behavior. We find that fluctuations are usually much larger than expected based on simplifying statistical assumptions (e.g., independence and lack of correlations between observations).These simplifications appear also in usual statistical tests so that the large fluctuations can be erroneously interpreted as a falsification of the law. Instead, here we argue that linguistic laws are only meaningful (falsifiable) if accompanied by a model for which the fluctuations can be computed (e.g., a generative model of the text). The large fluctuations we report show that the constraints imposed by linguistic laws on the creativity process of text generation are not as tight as one could expect.}}, 
local-url = {file://localhost/Users/tommy/Google%20Drive/gmu/_dissertation/references/download%202020-08-09/1502.03296.pdf}, 
year = {2015}
}
@inproceedings{lightlda, 
author = {Yuan, Jinhui and Gao, Fei and Ho, Qirong and Dai, Wei and Wei, Jinliang and Zheng, Xun and Xing, Eric P. and Liu, Tie-Yan and {Ma, and Wei-Ying}}, 
title = {{LightLDA: Big Topic Models on Modest Computer Clusters}}, 
booktitle = {Proceedings of the 24th International Conference on World Wide Web}, 
pages = {1351---1361}, 
local-url = {file://localhost/Users/tommy/Google%20Drive/gmu/_dissertation/references/2020-12-27/2736277.2741115.pdf}, 
year = {2015}
}
@misc{pazhayidam2015hyper, 
author = {GEORGE, CLINT PAZHAYIDAM}, 
title = {{LATENT DIRICHLET ALLOCATION: HYPERPARAMETER SELECTION AND APPLICATIONS TO ELECTRONIC DISCOVERY}}, 
shorttitle = {PhD Thesis}, 
publisher = {University of Florida}, 
local-url = {file://localhost/Users/tommy/Google%20Drive/gmu/_dissertation/references/download%202020-06-22/1e52bd981e20344c7295f88ed1802d2f56c1.pdf}, 
year = {2015}
}
@inproceedings{nguyen2015supervisedtm, 
author = {Nguyen, Thang and Boyd-Graber, Jordan and Lund, Jeffrey and Seppi, Kevin and Ringger, Eric}, 
title = {{Is Your Anchor Going Up or Down? Fast and Accurate Supervised Topic Models}}, 
booktitle = {Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL}, 
doi = {10.3115/v1/n15-1076}, 
pages = {746--755}, 
local-url = {file://localhost/Users/tommy/Google%20Drive/gmu/_dissertation/references/download%202020-06-22/N15-1076.pdf}, 
year = {2015}
}
@article{10.1103/physrevx.5.011007, 
title = {{High-Reproducibility and High-Accuracy Method for Automated Topic Classification}}, 
author = {Lancichinetti, Andrea and Sirer, M. Irmak and Wang, Jane X. and Acuna, Daniel and Körding, Konrad and Amaral, Luís A. Nunes}, 
journal = {Physical Review X}, 
doi = {10.1103/physrevx.5.011007}, 
abstract = {{Much of human knowledge sits in large databases of unstructured text. Leveraging this knowledge requires algorithms that extract and record metadata on unstructured text documents. Assigning topics to documents will enable intelligent searching, statistical characterization, and meaningful classification. Latent Dirichlet allocation (LDA) is the state of the art in topic modeling. Here, we perform a systematic theoretical and numerical analysis that demonstrates that current optimization techniques for LDA often yield results that are not accurate in inferring the most suitable model parameters. Adapting approaches from community detection in networks, we propose a new algorithm that displays high reproducibility and high accuracy and also has high computational efficiency. We apply it to a large set of documents in the English Wikipedia and reveal its hierarchical structure.}}, 
pages = {011007}, 
number = {1}, 
volume = {5}, 
local-url = {file://localhost/Users/tommy/Google%20Drive/gmu/_dissertation/references/2020-12-31/PhysRevX.5.011007.pdf}, 
year = {2015}
}
@inproceedings{roder2015coherence, 
author = {Röder, Michael and Both, Andreas and Hinneburg, Alexander}, 
title = {{Exploring the Space of Topic Coherence Measures}}, 
booktitle = {Proceedings of the eighth ACM international conference on Web search and data mining}, 
isbn = {9781450333177}, 
doi = {10.1145/2684822.2685324}, 
abstract = {{Quantifying the coherence of a set of statements is a long standing problem with many potential applications that has attracted researchers from different sciences. The special case of measuring coherence of topics has been recently studied to remedy the problem that topic models give no guaranty on the interpretablity of their output. Several benchmark datasets were produced that record human judgements of the interpretability of topics. We are the first to propose a framework that allows to construct existing word based coherence measures as well as new ones by combining elementary components. We conduct a systematic search of the space of coherence measures using all publicly available topic relevance data for the evaluation. Our results show that new combinations of components outperform existing measures with respect to correlation to human ratings. nFinally, we outline how our results can be transferred to further applications in the context of text mining, information retrieval and the world wide web.}}, 
pages = {399--408}, 
local-url = {file://localhost/Users/tommy/Google%20Drive/gmu/_dissertation/references/2020-12-26/2684822.2685324.pdf}, 
year = {2015}
}
@inproceedings{schnabel2015eval, 
author = {Schnabel, Tobias and Labutov, Igor and Mimno, David and Joachims, Thorsten}, 
title = {{Evaluation methods for unsupervised word embeddings}}, 
booktitle = {Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing}, 
local-url = {file://localhost/Users/tommy/Google%20Drive/gmu/_dissertation/references/download%202020-06-22/D15-1036.pdf}, 
year = {2015}
}
@article{zhao2015heuristic, 
title = {{A heuristic approach to determine an appropriate number of topics in topic modeling}}, 
author = {Zhao, Weizhong and Chen, James J and Perkins, Roger and Liu, Zhichao and Ge, Weigong and Ding, Yijun and Zou, Wen}, 
journal = {BMC Bioinformatics}, 
doi = {10.1186/1471-2105-16-s13-s8}, 
pmid = {26424364}, 
abstract = {{Topic modelling is an active research field in machine learning. While mainly used to build models from unstructured textual data, it offers an effective means of data mining where samples represent documents, and different biological endpoints or omics data represent words. Latent Dirichlet Allocation (LDA) is the most commonly used topic modelling method across a wide number of technical fields. However, model development can be arduous and tedious, and requires burdensome and systematic sensitivity studies in order to find the best set of model parameters. Often, time-consuming subjective evaluations are needed to compare models. Currently, research has yielded no easy way to choose the proper number of topics in a model beyond a major iterative approach. Based on analysis of variation of statistical perplexity during topic modelling, a heuristic approach is proposed in this study to estimate the most appropriate number of topics. Specifically, the rate of perplexity change (RPC) as a function of numbers of topics is proposed as a suitable selector. We test the stability and effectiveness of the proposed method for three markedly different types of grounded-truth datasets: Salmonella next generation sequencing, pharmacological side effects, and textual abstracts on computational biology and bioinformatics (TCBB) from PubMed. The proposed RPC-based method is demonstrated to choose the best number of topics in three numerical experiments of widely different data types, and for databases of very different sizes. The work required was markedly less arduous than if full systematic sensitivity studies had been carried out with number of topics as a parameter. We understand that additional investigation is needed to substantiate the method's theoretical basis, and to establish its generalizability in terms of dataset characteristics.}}, 
pages = {S8}, 
number = {Suppl 13}, 
volume = {16}, 
local-url = {file://localhost/Users/tommy/Google%20Drive/gmu/_dissertation/references/2020-12-26/1471-2105-16-S13-S8.pdf}, 
year = {2015}
}
@inproceedings{tang2014limits, 
author = {Tang, Jian and Meng, Zhaoshi and Nguyen, XuanLong and Mei, Qiaozhu and Zhang, Ming}, 
title = {{Understanding the Limiting Factors of Topic Modeling via Posterior Contraction Analysis}}, 
booktitle = {Proceedings of the 31 st International Conference on Machine Learning}, 
url = {http://proceedings.mlr.press/v32/tang14.pdf}, 
abstract = {{Topic models such as the latent Dirichlet allocation (LDA) have become a standard staple in
the modeling toolbox of machine learning. They
have been applied to a vast variety of data sets,
contexts, and tasks to varying degrees of success.
However, to date there is almost no formal theory
explicating the LDA’s behavior, and despite its
familiarity there is very little systematic analysis
of and guidance on the properties of the data that
affect the inferential performance of the model.
This paper seeks to address this gap, by providing
a systematic analysis of factors which characterize the LDA’s performance. We present theorems
elucidating the posterior contraction rates of the
topics as the amount of data increases, and a thorough supporting empirical study using synthetic
and real data sets, including news and web-based
articles and tweet messages. Based on these results we provide practical guidance on how to
identify suitable data sets for topic models, and
how to specify particular model parameters.}}, 
local-url = {file://localhost/Users/tommy/Google%20Drive/gmu/_dissertation/references/download%202020-07-27/tang14.pdf}, 
year = {2014}
}
@inproceedings{nguyen2014sometimes, 
author = {Nguyen, Viet-An and Boyd-Graber, Jordan and Resnik, Philip}, 
title = {{Sometimes Average is Best: The Importance of Averaging for Prediction using MCMC Inference in Topic Modeling}}, 
booktitle = {Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)}, 
doi = {10.3115/v1/d14-1182}, 
pages = {1752--1757}, 
local-url = {file://localhost/Users/tommy/Google%20Drive/gmu/_dissertation/references/2020-12-29/round2/D14-1182.pdf}, 
year = {2014}
}
@article{vulic2014crosslang, 
title = {{Probabilistic Models of Cross-Lingual Semantic Similarity in Context Based on Latent Cross-Lingual Concepts Induced from Comparable Data}}, 
author = {Vulić, Ivan and Moens, Marie-Francine}, 
doi = {10.3115/v1/d14-1040}, 
pages = {349--362}, 
local-url = {file://localhost/Users/tommy/Google%20Drive/gmu/_dissertation/references/download%202020-06-22/VulicMoensEMNLP2014.pdf}, 
year = {2014}
}
@inproceedings{lau2014machine, 
author = {Lau, Jey Han and Newman, David and Baldwin, Timothy}, 
title = {{Machine Reading Tea Leaves: Automatically Evaluating Topic Coherence and Topic Model Quality}}, 
booktitle = {Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics}, 
doi = {10.3115/v1/e14-1056}, 
pages = {530--539}, 
local-url = {file://localhost/Users/tommy/Google%20Drive/gmu/_dissertation/references/2020-12-29/round2/E14-1056.pdf}, 
year = {2014}
}
@article{hu2014interactive, 
title = {{Interactive topic modeling}}, 
author = {Hu, Yuening and Boyd-Graber, Jordan and Satinoff, Brianna and Smith, Alison}, 
journal = {Machine Learning}, 
issn = {0885-6125}, 
doi = {10.1007/s10994-013-5413-0}, 
abstract = {{Topic models are a useful and ubiquitous tool for understanding large corpora. However, topic models are not perfect, and for many users in computational social science, digital humanities, and information studies—who are not machine learning experts—existing models and frameworks are often a “take it or leave it” proposition. This paper presents a mechanism for giving users a voice by encoding users’ feedback to topic models as correlations between words into a topic model. This framework, interactive topic modeling (itm), allows untrained users to encode their feedback easily and iteratively into the topic models. Because latency in interactive systems is crucial, we develop more efficient inference algorithms for tree-based topic models. We validate the framework both with simulated and real users.}}, 
pages = {423--469}, 
number = {3}, 
volume = {95}, 
local-url = {file://localhost/Users/tommy/Google%20Drive/gmu/_dissertation/references/2020-12-29/round2/Hu2014_Article_InteractiveTopicModeling.pdf}, 
year = {2014}
}
@inproceedings{smith2014viz, 
author = {Smith, Alison and Chuang, Jason and Hu, Yuening and Boyd-Graber, Jordan and Findlater, Leah}, 
title = {{Concurrent Visualization of Relationships between Words and Topics in Topic Models}}, 
booktitle = {Proceedings of the Workshop on Interactive Language Learning, Visualization, and Interfaces}, 
doi = {10.3115/v1/w14-3112}, 
pages = {79--82}, 
local-url = {file://localhost/Users/tommy/Google%20Drive/gmu/_dissertation/references/download%202020-06-22/W14-3112.pdf}, 
year = {2014}
}
@article{wang2014translation, 
title = {{A Topic-Based Reordering Model for Statistical Machine Translation}}, 
author = {Wang, Xing and Xiong, Deyi and Zhang, Min and Hong, Yu and Yao, Jianmin}, 
issn = {1865-0929}, 
doi = {10.1007/978-3-662-45924-9\_37}, 
abstract = {{Reordering models are one of essential components of statistical machine translation. In this paper, we propose a topic-based reordering model to predict orders for neighboring blocks by capturing topic-sensitive reordering patterns. We automatically learn reordering examples from bilingual training data, which are associated with document-level and word-level topic information induced by LDA topic model. These learned reordering examples are used as evidences to train a topic-based reordering model that is built on a maximum entropy (MaxEnt) classifier. We conduct large-scale experiments to validate the effectiveness of the proposed topic-based reordering model on the NIST Chinese-to-English translation task. Experimental results show that our topic-based reordering model achieves significant performance improvement over the conventional reordering model using only lexical information.}}, 
pages = {414--421}, 
local-url = {file://localhost/Users/tommy/Google%20Drive/gmu/_dissertation/references/download%202020-06-22/167.pdf}, 
year = {2014}
}
@inproceedings{roberts2013stm, 
author = {Roberts, Margaret E. and Stewart, Brandon M. and Tingley, Dustin and Airoldi, Edoardo M.}, 
title = {{The Structural Topic Model and Applied Social Science}}, 
local-url = {file://localhost/Users/tommy/Google%20Drive/gmu/_dissertation/references/download%202020-06-23/stmnips2013.pdf}, 
year = {2013}
}
@inproceedings{nguyen2013regression, 
author = {Nguyen, Viet-An and Boyd-Graber, Jordan and Resnik, Philip}, 
title = {{Lexical and Hierarchical Topic Regression}}, 
booktitle = {Advances in Neural Information Processing Systems}, 
local-url = {file://localhost/Users/tommy/Google%20Drive/gmu/_dissertation/references/download%202020-06-22/5163-lexical-and-hierarchical-topic-regression.pdf}, 
year = {2013}
}
@article{miller2013simple, 
title = {{A simple example of Dirichlet process mixture inconsistency for the number of components}}, 
author = {Miller, Jeffrey W. and Harrison, Matthew T.}, 
journal = {Advances in neural information processing systems}, 
pages = {199---206}, 
volume = {26}, 
local-url = {file://localhost/Users/tommy/Google%20Drive/gmu/_dissertation/references/2020-12-29/dirichlet%20process%20inconsistency.pdf}, 
year = {2013}
}
@inproceedings{arora2013, 
author = {Arora, Sanjeev and Ge, Rong and Halpern, Yoni and Mimno, David and Moitra, Ankur and Sontag, David and Wu, Yichen and Zhu, Michael}, 
title = {{A Practical Algorithm for Topic Modeling with Provable Guarantees}}, 
booktitle = {Proceedings of the 30th International Conference on Machine Learning}, 
pages = {280–288}, 
publisher = {PLMR}, 
local-url = {file://localhost/Users/tommy/Google%20Drive/gmu/_dissertation/references/2020-12-31/arora13.pdf}, 
year = {2013}
}
@article{mimno2012sparse, 
title = {{Sparse Stochastic Inference for Latent Dirichlet allocation}}, 
author = {Mimno, David and Hoffman, Matt and Blei, David}, 
journal = {arXiv}, 
eprint = {1206.6425}, 
abstract = {{We present a hybrid algorithm for Bayesian topic models that combines the efficiency of sparse Gibbs sampling with the scalability of online stochastic inference. We used our algorithm to analyze a corpus of 1.2 million books (33 billion words) with thousands of topics. Our approach reduces the bias of variational inference and generalizes to many Bayesian hidden-variable models.}}, 
local-url = {file://localhost/Users/tommy/Google%20Drive/gmu/_dissertation/references/download%202020-06-22/1206.6425.pdf}, 
year = {2012}
}
@inproceedings{socher2012rnn, 
author = {Socher, Richard and Huval, Brody and Manning, Christopher D. and Ng, Andrew Y.}, 
title = {{Semantic Compositionality through Recursive Matrix-Vector Spaces}}, 
booktitle = {Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning}, 
local-url = {file://localhost/Users/tommy/Google%20Drive/gmu/_dissertation/references/download%202020-06-22/D12-1110.pdf}, 
year = {2012}
}
@article{yahoolda, 
title = {{Scalable inference in latent variable models}}, 
author = {Ahmed, Amr and Aly, Moahmed and Gonzalez, Joseph and Narayanamurthy, Shravan and Smola, Alexander J.}, 
doi = {10.1145/2124295.2124312}, 
abstract = {{Latent variable techniques are pivotal in tasks ranging from predicting user click patterns and targeting ads to organizing the news and managing user generated content. Latent variable techniques like topic modeling, clustering, and subspace estimation provide substantial insight into the latent structure of complex data with little or no external guidance making them ideal for reasoning about large-scale, rapidly evolving datasets. Unfortunately, due to the data dependencies and global state introduced by latent variables and the iterative nature of latent variable inference, latent-variable techniques are often prohibitively expensive to apply to large-scale, streaming datasets. In this paper we present a scalable parallel framework for efficient inference in latent variable models over streaming web-scale data. Our framework addresses three key challenges: 1) synchronizing the global state which includes global latent variables (e.g., cluster centers and dictionaries); 2) efficiently storing and retrieving the large local state which includes the data-points and their corresponding latent variables (e.g., cluster membership); and 3) sequentially incorporating streaming data (e.g., the news). We address these challenges by introducing: 1) a novel delta-based aggregation system with a bandwidth-efficient communication protocol; 2) schedule-aware out-of-core storage; and 3) approximate forward sampling to rapidly incorporate new data. We demonstrate state-of-the-art performance of our framework by easily tackling datasets two orders of magnitude larger than those addressed by the current state-of-the-art. Furthermore, we provide an optimized and easily customizable open-source implementation of the framework1.}}, 
pages = {123--132}, 
local-url = {file://localhost/Users/tommy/Google%20Drive/gmu/_dissertation/references/2020-12-27/2124295.2124312.pdf}, 
year = {2012}
}
@article{asuncion2012smoothing, 
title = {{On Smoothing and Inference for Topic Models}}, 
author = {Asuncion, Arthur and Welling, Max and Smyth, Padhraic and Teh, Yee Whye}, 
journal = {arXiv preprint arXiv:1205.2662}, 
local-url = {file://localhost/Users/tommy/Google%20Drive/gmu/_dissertation/references/2020-12-29/round2/1205.2662.pdf}, 
year = {2012}
}
@inproceedings{taddy2012, 
author = {Taddy, Matthew A.}, 
title = {{On Estimation and Selection for Topic Models}}, 
booktitle = {Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics}, 
local-url = {file://localhost/Users/tommy/Google%20Drive/gmu/_dissertation/references/2020-12-31/taddy12.pdf}, 
year = {2012}
}
@inproceedings{jagarlamudi2012transfer, 
author = {Jagarlamudi, Jagadeesh and Udupa, Raghavendra and III, Hal Daume}, 
title = {{Incorporating Lexical Priors into Topic Models}}, 
booktitle = {Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics}, 
local-url = {file://localhost/Users/tommy/Documents/Papers%20Library/lexical_priors.pdf}, 
year = {2012}
}
@article{teh2006hdp, 
title = {{Hierarchical Dirichlet Processes}}, 
author = {Teh, Yee Whye and Jordan, Michael I and Beal, Matthew J and Blei, David M}, 
journal = {Journal of the American Statistical Association}, 
issn = {0162-1459}, 
doi = {10.1198/016214506000000302}, 
abstract = {{We consider problems involving groups of data where each observation within a group is a draw from a mixture model and where it is desirable to share mixture components between groups. We assume that the number of mixture components is unknown a priori and is to be inferred from the data. In this setting it is natural to consider sets of Dirichlet processes, one for each group, where the well-known clustering property of the Dirichlet process provides a nonparametric prior for the number of mixture components within each group. Given our desire to tie the mixture models in the various groups, we consider a hierarchical model, specifically one in which the base measure for the child Dirichlet processes is itself distributed according to a Dirichlet process. Such a base measure being discrete, the child Dirichlet processes necessarily share atoms. Thus, as desired, the mixture models in the different groups necessarily share mixture components. We discuss representations of hierarchical Dirichlet processes in terms of a stick-breaking process, and a generalization of the Chinese restaurant process that we refer to as the “Chinese restaurant franchise.” We present Markov chain Monte Carlo algorithms for posterior inference in hierarchical Dirichlet process mixtures and describe applications to problems in information retrieval and text modeling.}}, 
pages = {1566--1581}, 
number = {476}, 
volume = {101}, 
local-url = {file://localhost/Users/tommy/Google%20Drive/gmu/_dissertation/references/2020-12-25/Hierarchical%20Dirichlet%20Processes.pdf}, 
year = {2012}
}
@inproceedings{stevens2012coherence, 
author = {Stevens, Keith and Kegelmeyer, Philip and Andrzejewski, David and Buttler, David}, 
title = {{Exploring Topic Coherence over many models and many topics}}, 
booktitle = {Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning}, 
pages = {952---961}, 
local-url = {file://localhost/Users/tommy/Google%20Drive/gmu/_dissertation/references/2020-12-26/D12-1087.pdf}, 
year = {2012}
}
@inproceedings{vulic2012bilinguallda, 
author = {Vulic, Ivan and Moens, Marie-Francine}, 
title = {{Detecting Highly Confident Word Translations from Comparable Corpora without Any Prior Knowledge}}, 
booktitle = {Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics}, 
local-url = {file://localhost/Users/tommy/Google%20Drive/gmu/_dissertation/references/download%202020-06-22/VulicMoensEACL2012final.pdf}, 
year = {2012}
}
@article{wang2012dynamic, 
title = {{Continuous Time Dynamic Topic Models}}, 
author = {Wang, Chong and Blei, David and Heckerman, David}, 
journal = {arXiv}, 
eprint = {1206.3298}, 
abstract = {{In this paper, we develop the continuous time dynamic topic model (cDTM). The cDTM is a dynamic topic model that uses Brownian motion to model the latent topics through a sequential collection of documents, where a "topic" is a pattern of word use that we expect to evolve over the course of the collection. We derive an efficient variational approximate inference algorithm that takes advantage of the sparsity of observations in text, a property that lets us easily handle many time points. In contrast to the cDTM, the original discrete-time dynamic topic model (dDTM) requires that time be discretized. Moreover, the complexity of variational inference for the dDTM grows quickly as time granularity increases, a drawback which limits fine-grained discretization. We demonstrate the cDTM on two news corpora, reporting both predictive perplexity and the novel task of time stamp prediction.}}, 
local-url = {file://localhost/Users/tommy/Google%20Drive/gmu/_dissertation/references/download%202020-06-22/1206.3298.pdf}, 
year = {2012}
}
@article{goldwater2011zipf, 
title = {{Producing Power-Law Distributions and Damping Word Frequencies with Two-Stage Language Models}}, 
author = {Goldwater, Sharon and Griffiths, Thomas L. and Johnson, Mark}, 
journal = {Journal of Machine Learning Research}, 
journaltitle = {Journal of Machine Learning Research}, 
volume = {12}, 
local-url = {file://localhost/Users/tommy/Documents/Papers%20Library/Goldwater-Producing%20Power-Law%20Distributions%20and%20Damping%20Word%20Frequencies%20with%20Two-Stage%20Language%20Models-2011-Journal%20of%20Machine%20Learning%20Research.pdf}, 
year = {2011}
}
@inproceedings{mimno2011coherence, 
author = {Mimno, David and Wallach, Hanna M. and Talley, Edmund and Leenders, Miriam and McCallum, Andrew}, 
title = {{Optimizing Semantic Coherence in Topic Models}}, 
booktitle = {Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing}, 
local-url = {file://localhost/Users/tommy/Google%20Drive/gmu/_dissertation/references/download%202020-06-22/D11-1024.pdf}, 
year = {2011}
}
@inproceedings{musat2011concept, 
author = {Musat, Claudiu Cristian and Velcin, Julien and Trausan-Matu, Stefan and Rizoiu, Marian-Andrei}, 
title = {{Improving Topic Evaluation Using Conceptual Knowledge}}, 
booktitle = {Proceedings  of  the  Twenty-Second  International  Joint  Conference  on  Artificial  Intelligence}, 
local-url = {file://localhost/Users/tommy/Google%20Drive/gmu/_dissertation/references/download%202020-06-22/3010-16166-1-PB.pdf}, 
year = {2011}
}
@inproceedings{vulic2011translation, 
author = {Vulic, Ivan and Smet, Wim De and Moens, Marie-Francine}, 
title = {{Identifying Word Translations from Comparable Corpora Using Latent Topic Models}}, 
booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies}, 
local-url = {file://localhost/Users/tommy/Google%20Drive/gmu/_dissertation/references/download%202020-06-22/VulicDeSmetMoensACL2011.pdf}, 
year = {2011}
}
@article{vulic2011cross, 
title = {{Cross-Language Information Retrieval with Latent Topic Models Trained on a Comparable Corpus}}, 
author = {Vulić, Ivan and Smet, Wim De and Moens, Marie-Francine}, 
issn = {0302-9743}, 
doi = {10.1007/978-3-642-25631-8\_4}, 
abstract = {{In this paper we study cross-language information retrieval using a bilingual topic model trained on comparable corpora such as Wikipedia articles. The bilingual Latent Dirichlet Allocation model (BiLDA) creates an interlingual representation, which can be used as a translation resource in many different multilingual settings as comparable corpora are available for many language pairs. The probabilistic interlingual representation is incorporated in a statistical language model for information retrieval. Experiments performed on the English and Dutch test datasets of the CLEF 2001-2003 CLIR campaigns show the competitive performance of our approach compared to cross-language retrieval methods that rely on pre-existing translation dictionaries that are hand-built or constructed based on parallel corpora.}}, 
pages = {37--48}, 
local-url = {file://localhost/Users/tommy/Google%20Drive/gmu/_dissertation/references/download%202020-06-22/VulicDeSmetMoensAIRS2011.pdf}, 
year = {2011}
}
@inproceedings{mimno2011checking, 
author = {Mimno, David and Blei, David}, 
title = {{Bayesian Checking for Topic Models}}, 
booktitle = {Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing}, 
local-url = {file://localhost/Users/tommy/Google%20Drive/gmu/_dissertation/references/download%202020-06-22/D11-1021.pdf}, 
year = {2011}
}
@article{hoffman2010online, 
title = {{Online Learning for Latent Dirichlet Allocation}}, 
author = {Hoffman, Matthew D. and Blei, David M. and Bach, Francis}, 
journal = {advances in neural information processing systems}, 
pages = {856---864}, 
volume = {23}, 
local-url = {file://localhost/Users/tommy/Google%20Drive/gmu/_dissertation/references/2020-12-29/round2/NIPS-2010-online-learning-for-latent-dirichlet-allocation-Paper.pdf}, 
year = {2010}
}
@inproceedings{zhang2010cross, 
author = {Zhang, Duo and Mei, Qiaozhu and Zhai, ChengXiang}, 
title = {{Cross-Lingual Latent Topic Extraction}}, 
booktitle = {Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics}, 
local-url = {file://localhost/Users/tommy/Google%20Drive/gmu/_dissertation/references/download%202020-06-22/P10-1115.pdf}, 
year = {2010}
}
@inproceedings{newman2010coherence, 
author = {Newman, David and Lau, Jey Han and Grieser, Karl and Baldwin, Timothy}, 
title = {{Automatic Evaluation of Topic Coherence}}, 
booktitle = {Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL}, 
local-url = {file://localhost/Users/tommy/Google%20Drive/gmu/_dissertation/references/download%202020-06-22/N10-1012.pdf}, 
year = {2010}
}
@inproceedings{alsumait2009, 
author = {AlSumait, Loulwah and Barbará, Daniel and Gentle, James and Domeniconi, Carlotta}, 
title = {{Topic Significance Ranking of LDA Generative Models}}, 
booktitle = {Joint European Conference on Machine Learning and Knowledge Discovery in Databases}, 
abstract = {{Topic models, like Latent Dirichlet Allocation (LDA), have been recently used to automatically generate text corpora topics, and to subdivide the corpus words among those topics. However, not all the estimated topics are of equal importance or correspond to genuine themes of the domain. Some of the topics can be a collection of irrelevant words, or represent insignificant themes. Current approaches to topic modeling perform manual examination to find meaningful topics. This paper presents the first automated unsupervised analysis of LDA models to identify junk topics from legitimate ones, and to rank the topic significance. Basically, the distance between a topic distribution and three definitions of “junk distribution” is computed using a variety of measures, from which an expressive figure of the topic significance is implemented using 4-phase Weighted Combination approach. Our experiments on synthetic and benchmark datasets show the effectiveness of the proposed approach in ranking the topic significance.}}, 
pages = {67--82}, 
local-url = {file://localhost/Users/tommy/Google%20Drive/gmu/_dissertation/references/download%202020-06-22/AlSumait2009_Chapter_TopicSignificanceRankingOfLDAG.pdf}, 
year = {2009}
}
@inproceedings{park2009sensitivity, 
author = {Park, Laurence A. F. and Ramamohanarao, Kotagiri}, 
title = {{The Sensitivity of Latent Dirichlet Allocation for Information Retrieval}}, 
booktitle = {Joint European Conference on Machine Learning and Knowledge Discovery in Databases}, 
isbn = {9783642041730}, 
doi = {10.1007/978-3-642-04174-7\_12}, 
abstract = {{It has been shown that the use of topic models for Information retrieval provides an increase in precision when used in the appropriate form. Latent Dirichlet Allocation (LDA) is a generative topic model that allows us to model documents using a Dirichlet prior. Using this topic model, we are able to obtain a fitted Dirichlet parameter that provides the maximum likelihood for the document set. In this article, we examine the sensitivity of LDA with respect to the Dirichlet parameter when used for Information retrieval. We compare the topic model computation times, storage requirements and retrieval precision of fitted LDA to LDA with a uniform Dirichlet prior. The results show there there is no significant benefit of using fitted LDA over the LDA with a constant Dirichlet parameter, hence showing that LDA is insensitive with respect to the Dirichlet parameter when used for Information retrieval.}}, 
pages = {176--188}, 
local-url = {file://localhost/Users/tommy/Google%20Drive/gmu/_dissertation/references/download%202020-06-22/Park-Ramamohanarao2009_Chapter_TheSensitivityOfLatentDirichle.pdf}, 
year = {2009}
}
@inproceedings{wallach2009sup, 
author = {Wallach, Hanna M. and Mimno, David and McCallum, Andrew}, 
title = {{Supplementary Materials for “Rethinking LDA: Why Priors Matter”}}, 
local-url = {file://localhost/Users/tommy/Google%20Drive/gmu/_dissertation/references/download%202020-06-22/NIPS2009_0929_extra.pdf}, 
year = {2009}
}
@inproceedings{wallach2009rethinking, 
author = {Wallach, Hanna M. and Mimno, David and McCallum, Andrew}, 
title = {{Rethinking LDA: Why Priors Matter}}, 
booktitle = {Advances in neural information processing systems}, 
local-url = {file://localhost/Users/tommy/Google%20Drive/gmu/_dissertation/references/download%202020-06-22/3854-rethinking-lda-why-priors-matter.pdf}, 
year = {2009}
}
@inproceedings{mukherjee2009perf, 
author = {Mukherjee, Indraneel and Blei, David M.}, 
title = {{Relative Performance Guarantees for Approximate Inference in Latent Dirichlet Allocation}}, 
booktitle = {Advances in Neural Information Processing Systems}, 
local-url = {file://localhost/Users/tommy/Google%20Drive/gmu/_dissertation/references/download%202020-06-22/3455-relative-performance-guarantees-for-approximate-inference-in-latent-dirichlet-allocation.pdf}, 
year = {2009}
}
@inproceedings{chang2009tea, 
author = {Chang, Jonathan and Boyd-Graber, Jordan}, 
title = {{Reading Tea Leaves: How Humans Interpret Topic Models}}, 
booktitle = {Advances in neural information processing systems}, 
local-url = {file://localhost/Users/tommy/Google%20Drive/gmu/_dissertation/references/download%202020-06-22/3700-reading-tea-leaves-how-humans-interpret-topic-models.pdf}, 
year = {2009}
}
@inproceedings{mimno2009polylingual, 
author = {Mimno, David and Wallach, Hanna M. and Naradowsky, Jason and Smith, David A. and McCallum, Andrew}, 
title = {{Polylingual Topic Models}}, 
booktitle = {Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing}, 
local-url = {file://localhost/Users/tommy/Google%20Drive/gmu/_dissertation/references/download%202020-06-22/D09-1092.pdf}, 
year = {2009}
}
@inproceedings{yan2009gpu, 
author = {Yan, Feng and Xu, Ningyi and Qi, Yuan}, 
title = {{Parallel Inference for Latent Dirichlet Allocation on Graphics Processing Units}}, 
booktitle = {Advances in Neural Information Processing Systems}, 
abstract = {{The recent emergence of Graphics Processing Units (GPUs) as general-purpose parallel computing devices provides us with new opportunities to develop scalable learning methods for massive data. In this work, we consider the problem of parallelizing two inference methods on GPUs for latent Dirichlet Allocation (LDA) models, collapsed Gibbs sampling (CGS) and collapsed variational Bayesian (CVB). To address limited memory constraints on GPUs, we propose a novel data partitioning scheme that effectively reduces the memory cost. Furthermore, the partitioning scheme balances the computational cost on each multiprocessor and enables us to easily avoid memory access conflicts. We also use data streaming to handle extremely large datasets. Extensive experiments showed that our parallel inference methods consistently produced LDA models with the same predictive power as sequential training methods did but with 26x speedup for CGS and 196x speedup for CVB on a GPU with 30 multiprocessors; actually the speedup is almost linearly scalable with the number of multiprocessors available. The proposed partitioning scheme and data streaming can be easily ported to many other}}, 
volume = {22}, 
local-url = {file://localhost/Users/tommy/Documents/Papers%20Library/3788-parallel-inference-for-latent-dirichlet-allocation-on-graphics-processing-units.pdf}, 
year = {2009}
}
@article{buntine2009likelihood, 
title = {{Lecture Notes in Computer Science}}, 
author = {Buntine, Wray}, 
issn = {0302-9743}, 
doi = {10.1007/978-3-642-05224-8\_6}, 
abstract = {{Topic models are a discrete analogue to principle component analysis and independent component analysis that model topic at the word level within a document. They have many variants such as NMF, PLSI and LDA, and are used in many fields such as genetics, text and the web, image analysis and recommender systems. However, only recently have reasonable methods for estimating the likelihood of unseen documents, for instance to perform testing or model comparison, become available. This paper explores a number of recent methods, and improves their theory, performance, and testing.}}, 
pages = {51--64}, 
local-url = {file://localhost/Users/tommy/Google%20Drive/gmu/_dissertation/references/download%202020-06-22/10.1.1.332.4613.pdf}, 
year = {2009}
}
@inproceedings{andrzejewski2009latent, 
author = {Andrzejewski, David and Zhu, Xiaojin}, 
title = {{Latent Dirichlet Allocation with Topic-in-Set Knowledge}}, 
booktitle = {Proceedings of the NAACL HLT 2009 Workshop on Semi-Supervised Learning for Natural Language Processing}, 
pages = {43---48}, 
local-url = {file://localhost/Users/tommy/Google%20Drive/gmu/_dissertation/references/2020-12-29/round2/W09-2206.pdf}, 
year = {2009}
}
@inproceedings{ramage2009labled, 
author = {Ramage, Daniel and Hall, David and Nallapati, Ramesh and Manning, Christopher D.}, 
title = {{Labeled LDA: A supervised topic model for credit attribution in multi-labeled corpora}}, 
booktitle = {Proceedings of the 2009 conference on empirical methods in natural language processing}, 
pages = {248---256}, 
local-url = {file://localhost/Users/tommy/Google%20Drive/gmu/_dissertation/references/2020-12-29/round2/D09-1026.pdf}, 
year = {2009}
}
@article{andrzejewski2009, 
title = {{Incorporating domain knowledge into topic modeling via Dirichlet Forest priors}}, 
author = {Andrzejewski, David and Zhu, Xiaojin and Craven, Mark}, 
doi = {10.1145/1553374.1553378}, 
pmid = {20871805}, 
abstract = {{Users of topic modeling methods often have knowledge about the composition of words that should have high or low probability in various topics. We incorporate such domain knowledge using a novel Dirichlet Forest prior in a Latent Dirichlet Allocation framework. The prior is a mixture of Dirichlet tree distributions with special structures. We present its construction, and inference via collapsed Gibbs sampling. Experiments on synthetic and real datasets demonstrate our model's ability to follow and generalize beyond user-specified domain knowledge.}}, 
pages = {25--32}, 
local-url = {file://localhost/Users/tommy/Google%20Drive/gmu/_dissertation/references/2020-12-29/round2/1553374.1553378.pdf}, 
year = {2009}
}
@article{wallach2009eval, 
title = {{Evaluation methods for topic models}}, 
author = {Wallach, Hanna M. and Murray, Iain and Salakhutdinov, Ruslan and Mimno, David}, 
doi = {10.1145/1553374.1553515}, 
abstract = {{A natural evaluation metric for statistical topic models is the probability of held-out documents given a trained model. While exact computation of this probability is intractable, several estimators for this probability have been used in the topic modeling literature, including the harmonic mean method and empirical likelihood method. In this paper, we demonstrate experimentally that commonly-used methods are unlikely to accurately estimate the probability of held-out documents, and propose two alternative methods that are both accurate and efficient.}}, 
pages = {1105--1112}, 
local-url = {file://localhost/Users/tommy/Google%20Drive/gmu/_dissertation/references/download%202020-06-22/1553374.1553515.pdf}, 
year = {2009}
}
@article{yao2009streaming, 
title = {{Efficient methods for topic model inference on streaming document collections}}, 
author = {Yao, Limin and Mimno, David and McCallum, Andrew}, 
doi = {10.1145/1557019.1557121}, 
abstract = {{Topic models provide a powerful tool for analyzing large text collections by representing high dimensional data in a low dimensional subspace. Fitting a topic model given a set of training documents requires approximate inference techniques that are computationally expensive. With today's large-scale, constantly expanding document collections, it is useful to be able to infer topic distributions for new documents without retraining the model. In this paper, we empirically evaluate the performance of several methods for topic inference in previously unseen documents, including methods based on Gibbs sampling, variational inference, and a new method inspired by text classification. The classification-based inference method produces results similar to iterative inference methods, but requires only a single matrix multiplication. In addition to these inference methods, we present SparseLDA, an algorithm and data structure for evaluating Gibbs sampling distributions. Empirical results indicate that SparseLDA can be approximately 20 times faster than traditional LDA and provide twice the speedup of previously published fast sampling methods, while also using substantially less memory.}}, 
pages = {937--946}, 
local-url = {file://localhost/Users/tommy/Google%20Drive/gmu/_dissertation/references/download%202020-06-22/1557019.1557121.pdf}, 
year = {2009}
}
@article{newman2009distributed, 
title = {{Distributed Algorithms for Topic Models}}, 
author = {Newman, David and Asuncion, Arthur and Smyth, Padhraic and Welling, Max}, 
journal = {Journal of Machine Learnign Research}, 
journaltitle = {Journal of Machine Learning Research}, 
number = {8}, 
volume = {10}, 
local-url = {file://localhost/Users/tommy/Documents/Papers%20Library/newman09a.pdf}, 
year = {2009}
}
@article{alsumait2008, 
title = {{On-Line LDA: Adaptive Topic Models for Mining Text Streams with Applications to Topic Detection and Tracking}}, 
author = {AlSumait, Loulwah and Barbará, Daniel and Domeniconi, Carlotta}, 
journal = {2008 Eighth IEEE International Conference on Data Mining}, 
doi = {10.1109/icdm.2008.140}, 
abstract = {{This paper presents Online Topic Model (OLDA), a topic model that automatically captures the thematic patterns and identifies emerging topics of text streams and their changes over time. Our approach allows the topic modeling framework, specifically the Latent Dirichlet Allocation (LDA) model, to work in an online fashion such that it incrementally builds an up-to-date model (mixture of topics per document and mixture of words per topic) when a new document (or a set of documents) appears. A solution based on the Empirical Bayes method is proposed. The idea is to incrementally update the current model according to the information inferred from the new stream of data with no need to access previous data. The dynamics of the proposed approach also provide an efficient mean to track the topics over time and detect the emerging topics in real time. Our method is evaluated both qualitatively and quantitatively using benchmark datasets. In our experiments, the OLDA has discovered interesting patterns by just analyzing a fraction of data at a time. Our tests also prove the ability of OLDA to align the topics across the epochs with which the evolution of the topics over time is captured. The OLDA is also comparable to, and sometimes better than, the original LDA in predicting the likelihood of unseen documents.}}, 
pages = {3--12}, 
local-url = {file://localhost/Users/tommy/Google%20Drive/gmu/_dissertation/references/2020-12-28/04781095.pdf}, 
year = {2008}
}
@inproceedings{newman2008distributed, 
author = {Newman, David and Asuncion, Arthur and Smyth, Padhraic and Welling, Max}, 
title = {{Distributed Inference for Latent Dirichlet Allocation}}, 
booktitle = {Advances in neural information processing systems}, 
local-url = {file://localhost/Users/tommy/Google%20Drive/gmu/_dissertation/references/download%202020-06-22/3330-distributed-inference-for-latent-dirichlet-allocation.pdf}, 
year = {2008}
}
@article{mcauliffe2007supervised, 
title = {{Supervised topic models}}, 
author = {McAuliffe, Jon and Blei, David}, 
journal = {Advances in neural information processing systems}, 
pages = {121---128}, 
volume = {20}, 
local-url = {file://localhost/Users/tommy/Google%20Drive/gmu/_dissertation/references/2020-12-25/NIPS-2007-supervised-topic-models-Paper.pdf}, 
year = {2007}
}
@article{douven2007coherence, 
title = {{Measuring coherence}}, 
author = {Douven, Igor and Meijs, Wouter}, 
journal = {Synthese}, 
issn = {0039-7857}, 
doi = {10.1007/s11229-006-9131-z}, 
abstract = {{This paper aims to contribute to our understanding of the notion of coherence by explicating in probabilistic terms, step by step, what seem to be our most basic intuitions about that notion, to wit, that coherence is a matter of hanging or fitting together, and that coherence is a matter of degree. A qualitative theory of coherence will serve as a stepping stone to formulate a set of quantitative measures of coherence, each of which seems to capture well the aforementioned intuitions. Subsequently it will be argued that one of those measures does better than the others in light of some more specific intuitions about coherence. This measure will be defended against two seemingly obvious objections.}}, 
pages = {405--425}, 
number = {3}, 
volume = {156}, 
local-url = {file://localhost/Users/tommy/Documents/Papers%20Library/Douven-Measuring%20coherence-2007-Synthese.pdf}, 
year = {2007}
}
@article{blei2007ctm, 
title = {{A correlated topic model of Science}}, 
author = {Blei, David M. and Lafferty, John D.}, 
journal = {The Annals of Applied Statistics}, 
issn = {1932-6157}, 
doi = {10.1214/07-aoas114}, 
abstract = {{Topic models, such as latent Dirichlet allocation (LDA), can be useful tools for the statistical analysis of document collections and other discrete data. The LDA model assumes that the words of each document arise from a mixture of topics, each of which is a distribution over the vocabulary. A limitation of LDA is the inability to model topic correlation even though, for example, a document about genetics is more likely to also be about disease than X-ray astronomy. This limitation stems from the use of the Dirichlet distribution to model the variability among the topic proportions. In this paper we develop the correlated topic model (CTM), where the topic proportions exhibit correlation via the logistic normal distribution [J. Roy. Statist. Soc. Ser. B 44 (1982) 139–177]. We derive a fast variational inference algorithm for approximate posterior inference in this model, which is complicated by the fact that the logistic normal is not conjugate to the multinomial. We apply the CTM to the articles from Science published from 1990–1999, a data set that comprises 57M words. The CTM gives a better fit of the data than LDA, and we demonstrate its use as an exploratory tool of large document collections.}}, 
pages = {17--35}, 
number = {1}, 
volume = {1}, 
local-url = {file://localhost/Users/tommy/Google%20Drive/gmu/_dissertation/references/download%202020-06-23/euclid.aoas.1183143727.pdf}, 
year = {2007}
}
@inproceedings{teh2007cvb, 
author = {Teh, Yee Whye and Newman, David and Welling, Max}, 
title = {{A Collapsed Variational Bayesian Inference A lgorithm for Latent Dirichlet Allocation}}, 
booktitle = {Advances in neural information processing systems}, 
local-url = {file://localhost/Users/tommy/Google%20Drive/gmu/_dissertation/references/download%202020-06-22/3113-a-collapsed-variational-bayesian-inference-algorithm-for-latent-dirichlet-allocation.pdf}, 
year = {2007}
}
@article{blei2006dtm, 
title = {{Dynamic topic models}}, 
author = {Blei, David M. and Lafferty, John D.}, 
doi = {10.1145/1143844.1143859}, 
abstract = {{A family of probabilistic time series models is developed to analyze the time evolution of topics in large document collections. The approach is to use state space models on the natural parameters of the multinomial distributions that represent the topics. Variational approximations based on Kalman filters and nonparametric wavelet regression are developed to carry out approximate posterior inference over the latent topics. In addition to giving quantitative, predictive models of a sequential corpus, dynamic topic models provide a qualitative window into the contents of a large document collection. The models are demonstrated by analyzing the OCR'ed archives of the journal Science from 1880 through 2000.}}, 
pages = {113--120}, 
local-url = {file://localhost/Users/tommy/Google%20Drive/gmu/_dissertation/references/2020-12-25/1143844.1143859.pdf}, 
year = {2006}
}
@article{griffiths2004scientific, 
title = {{Finding scientific topics}}, 
author = {Griffiths, T L and Steyvers, M}, 
journal = {Proceedings of the National Academy of Sciences}, 
issn = {0027-8424}, 
doi = {10.1073/pnas.0307752101}, 
pmid = {14872004}, 
pmcid = {PMC387300}, 
abstract = {{A first step in identifying the content of a document is determining which topics that document addresses. We describe a generative model for documents, introduced by Blei, Ng, and Jordan [Blei, D. M., Ng, A. Y. \& Jordan, M. I. (2003) J. Machine Learn. Res. 3, 993-1022], in which each document is generated by choosing a distribution over topics and then choosing each word in the document from a topic selected according to this distribution. We then present a Markov chain Monte Carlo algorithm for inference in this model. We use this algorithm to analyze abstracts from PNAS by using Bayesian model selection to establish the number of topics. We show that the extracted topics capture meaningful structure in the data, consistent with the class designations provided by the authors of the articles, and outline further applications of this analysis, including identifying “hot topics” by examining temporal dynamics and tagging abstracts to illustrate semantic content.}}, 
pages = {5228--5235}, 
number = {Supplement 1}, 
volume = {101}, 
local-url = {file://localhost/Users/tommy/Google%20Drive/gmu/_dissertation/references/download%202020-06-22/5228.full.pdf}, 
year = {2004}
}
@article{girolami2003plsilda, 
title = {{On an equivalence between PLSI and LDA}}, 
author = {Girolami, Mark and Kabán, Ata}, 
doi = {10.1145/860435.860537}, 
abstract = {{Latent Dirichlet Allocation (LDA) is a fully generative approach to language modelling which overcomes the inconsistent generative semantics of Probabilistic Latent Semantic Indexing (PLSI). This paper shows that PLSI is a maximum a posteriori estimated LDA model under a uniform Dirichlet prior, therefore the perceived shortcomings of PLSI can be resolved and elucidated within the LDA framework.}}, 
pages = {433--434}, 
local-url = {file://localhost/Users/tommy/Google%20Drive/gmu/_dissertation/references/download%202020-06-22/860435.860537.pdf}, 
year = {2003}
}
@article{blei2002lda, 
title = {{Latent Dirichlet Allocation}}, 
author = {Blei, David M. and Ng, Andrew Y. and Jordan, Michael I.}, 
journal = {Journal of Machine Learning Research}, 
volume = {3}, 
local-url = {file://localhost/Users/tommy/Google%20Drive/gmu/_dissertation/references/download%202020-06-22/blei03a.pdf}, 
year = {2003}
}
@article{minka2002, 
title = {{Expectation-propogation for the generative aspect model}}, 
author = {Minka, Thomas and Lafferty, John}, 
journal = {Uncertainty in Artificial Intelligence}, 
pages = {352–359}, 
local-url = {file://localhost/Users/tommy/Google%20Drive/gmu/_dissertation/references/2020-12-31/1301.0588.pdf}, 
year = {2002}
}
@article{papadimitrio2000lsa, 
title = {{Latent Semantic Indexing: A Probabilistic Analysis}}, 
author = {Papadimitriou, Christos H. and Raghavan, Prabhakar and Tamaki, Hisao and Vempala, Santosh}, 
journal = {Journal of Computer and System Sciences}, 
issn = {0022-0000}, 
doi = {10.1006/jcss.2000.1711}, 
abstract = {{Latent semantic indexing (LSI) is an information retrieval technique based on the spectral analysis of the term-document matrix, whose empirical success had heretofore been without rigorous prediction and explanation. We prove that, under certain conditions, LSI does succeed in capturing the underlying semantics of the corpus and achieves improved retrieval performance. We propose the technique of random projection as a way of speeding up LSI. We complement our theorems with encouraging experimental results. We also argue that our results may be viewed in a more general framework, as a theoretical basis for the use of spectral methods in a wider class of applications such as collaborative filtering.}}, 
pages = {217--235}, 
number = {2}, 
volume = {61}, 
local-url = {file://localhost/Users/tommy/Google%20Drive/gmu/_dissertation/references/download%202020-06-22/1-s2.0-S0022000000917112-main.pdf}, 
year = {2000}
}
@article{deerwester1990lsa, 
title = {{Indexing by latent semantic analysis}}, 
author = {Deerwester, Scott and Dumais, Susan T and Furnas, George W and Landauer, Thomas K and Harshman, Richard}, 
journal = {Journal of the American Society for Information Science}, 
issn = {0002-8231}, 
doi = {10.1002/(sici)1097-4571(199009)41:6<391::aid-asi1>3.0.co;2-9}, 
abstract = {{A new method for automatic indexing and retrieval is described. The approach is to take advantage of implicit higher-order structure in the association of terms with documents (“semantic structure”) in order to improve the detection of relevant documents on the basis of terms found in queries. The particular technique used is singular-value decomposition, in which a large term by document matrix is decomposed into a set of ca. 100 orthogonal factors from which the original matrix can be approximated by linear combination. Documents are represented by ca. 100 item vectors of factor weights. Queries are represented as pseudo-document vectors formed from weighted combinations of terms, and documents with supra-threshold cosine values are returned. Initial tests find this completely automatic method for retrieval to be promising. © 1990 John Wiley \& Sons, Inc.}}, 
pages = {391--407}, 
number = {6}, 
volume = {41}, 
local-url = {file://localhost/Users/tommy/Google%20Drive/gmu/_dissertation/references/2020-08-15/10.1.1.62.1152.pdf}, 
year = {1990}
}
@article{salton1975, 
title = {{A vector space model for automatic indexing}}, 
author = {Salton, G. and Wong, A. and Yang, C. S.}, 
journal = {Communications of the ACM}, 
issn = {0001-0782}, 
doi = {10.1145/361219.361220}, 
abstract = {{In a document retrieval, or other pattern matching environment where stored entities (documents) are compared with each other or with incoming patterns (search requests), it appears that the best indexing (property) space is one where each entity lies as far away from the others as possible; in these circumstances the value of an indexing system may be expressible as a function of the density of the object space; in particular, retrieval performance may correlate inversely with space density. An approach based on space density computations is used to choose an optimum indexing vocabulary for a collection of documents. Typical evaluation results are shown, demonstating the usefulness of the model.}}, 
pages = {613--620}, 
number = {11}, 
volume = {18}, 
local-url = {file://localhost/Users/tommy/Google%20Drive/gmu/_dissertation/references/2020-12-25/361219.361220.pdf}, 
year = {1975}
}

@article{rollinglda, 
year = {2021}, 
title = {{RollingLDA: An Update Algorithm of Latent Dirichlet Allocation to Construct Consistent Time Series from Textual Data}}, 
author = {Rieger, Jonas and Jentsch, Carsten and Rahnenfuhrer, Jorg}, 
url = {http://www.statistik.tu-dortmund.de/fileadmin/user\_upload/Lehrstuehle/IWuS/Forschung/rollinglda.pdf}, 
local-url = {file://localhost/Users/tommy/Documents/Papers%20Library/rollinglda-1.pdf}
}

@inproceedings{tmlda,
  title={TM-LDA: efficient online modeling of latent topic transitions in social media},
  author={Wang, Yu and Agichtein, Eugene and Benzi, Michele},
  booktitle={Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining},
  pages={123--131},
  year={2012}
}